lr <- 0.5
max_iter <- 500
noise_sd <- .1
batch_size <- 100
res_DP <- DP_descent(beta_init, X, y, lr, max_iter, batch_size, noise_sd)
res_Bark <- Bark_Descent(beta_init, X, y, lr, max_iter, batch_size, noise_sd)
plot(res_DP$params.MSEs, type = "l", col = "blue", lwd = 2,
xlab = "Iteration", ylab = "Loss",
main = "Params MSEs (DP Vs Barker's Private Descent)",
ylim = c(0, max(c(res_DP$params.MSEs, res_Bark$params.MSEs))))
lines(res_Bark$params.MSEs, col = "red", lwd = 2)
abline(h = mean((y - X %*% beta_true)^2)/2, col = "green", lty = 2)
legend("topright", legend = c("Gradient Descent", "Barker's GD", "Optimal Loss"),
col = c("blue", "red", "green"), lwd = 2)
plot(res_DP$losses, type = "l", col = "blue", lwd = 2,
xlab = "Iteration", ylab = "Loss",
main = "Pred Losses (DP Vs Barker's Private Descent)",
ylim = c(0, max(c(res_DP$losses, res_Bark$losses))))
lines(res_Bark$losses, col = "red", lwd = 2)
abline(h = mean((y - X %*% beta_true)^2)/2, col = "green", lty = 2)
legend("topright", legend = c("Gradient Descent", "Barker's GD", "Optimal Loss"),
col = c("blue", "red", "green"), lwd = 2)
### 7. Plotting the actual models
lr <- 0.5
max_iter <- 2000
noise_sd <- .1
batch_size <- 100
res_DP <- DP_descent(beta_init, X, y, lr, max_iter, batch_size, noise_sd)
res_Bark <- Bark_Descent(beta_init, X, y, lr, max_iter, batch_size, noise_sd)
plot(res_DP$params.MSEs, type = "l", col = "blue", lwd = 2,
xlab = "Iteration", ylab = "Loss",
main = "Params MSEs (DP Vs Barker's Private Descent)",
ylim = c(0, max(c(res_DP$params.MSEs, res_Bark$params.MSEs))))
lines(res_Bark$params.MSEs, col = "red", lwd = 2)
abline(h = mean((y - X %*% beta_true)^2)/2, col = "green", lty = 2)
legend("topright", legend = c("Gradient Descent", "Barker's GD", "Optimal Loss"),
col = c("blue", "red", "green"), lwd = 2)
plot(res_DP$losses, type = "l", col = "blue", lwd = 2,
xlab = "Iteration", ylab = "Loss",
main = "Pred Losses (DP Vs Barker's Private Descent)",
ylim = c(0, max(c(res_DP$losses, res_Bark$losses))))
lines(res_Bark$losses, col = "red", lwd = 2)
abline(h = mean((y - X %*% beta_true)^2)/2, col = "green", lty = 2)
legend("topright", legend = c("Gradient Descent", "Barker's GD", "Optimal Loss"),
col = c("blue", "red", "green"), lwd = 2)
solve(t(X)%*%X)%*%X%*%y
t(X)%*%X
plot(res_DP$params.MSEs, type = "l", col = "blue", lwd = 2,
xlab = "Iteration", ylab = "Loss",
main = "Params MSEs (DP Vs Barker's Private Descent)",
ylim = c(0, max(c(res_DP$params.MSEs, res_Bark$params.MSEs))))
lines(res_Bark$params.MSEs, col = "red", lwd = 2)
abline(h = mean((beta_true - solve(t(X)%*%X)%*%t(X)%*%y)^2)/2, col = "green", lty = 2)
legend("topright", legend = c("Gradient Descent", "Barker's GD", "Optimal Loss"),
col = c("blue", "red", "green"), lwd = 2)
plot(res_DP$losses, type = "l", col = "blue", lwd = 2,
xlab = "Iteration", ylab = "Loss",
main = "Pred Losses (DP Vs Barker's Private Descent)",
ylim = c(0, max(c(res_DP$losses, res_Bark$losses))))
lines(res_Bark$losses, col = "red", lwd = 2)
abline(h = mean((y - X %*% beta_true)^2)/2, col = "green", lty = 2)
legend("topright", legend = c("Gradient Descent", "Barker's GD", "Optimal Loss"),
col = c("blue", "red", "green"), lwd = 2)
clipped_grad_mse(beta_true, X, y)
Cap = 1
beta <- beta_true
residuals <- as.vector(X %*% beta - y)
gradient_matrix <- X * residuals
gradient_matrix <- apply(gradient_matrix, 1, clip_grad, Cap = Cap)
dim(gradient_matrix)
residuals <- as.vector(X %*% beta - y)
gradient_matrix <- X * residuals
gradient_matrix <- apply(t(gradient_matrix), 1, clip_grad, Cap = Cap)
colMeans(t(gradient_matrix))
t(gradient_matrix)
clipped_grad_mse <- function(beta, X, y, Cap = 1) {
residuals <- as.vector(X %*% beta - y)
gradient_matrix <- X * residuals
gradient_matrix <- apply(gradient_matrix, 1, clip_grad, Cap = Cap)
colMeans(t(gradient_matrix))
}
clipped_grad_mse(beta, X, y)
set.seed(123)
### 1. Generate synthetic linear regression dataset
n <- 1e3   # samples
p <- 100     # features
X <- matrix(rnorm(n * p), n, p)
beta_true <- rnorm(p, mean = 2, sd = 1)  # true coefficients
y <- X %*% beta_true + rnorm(n, sd = 0.5)
### 2. Loss function and gradient
mse_loss <- function(beta, X, y) {
residuals <- X %*% beta - y
mean(residuals^2) / 2  # divide by 2 for cleaner gradient form
}
grad_mse <- function(beta, X, y) {
residuals <- X %*% beta - y
t(X) %*% residuals / nrow(X)
}
clip_grad <- function(grad, Cap = 1){
div_factor <- max (Cap, sqrt(sum(grad^2)))
grad <- grad / div_factor
return(grad)
}
clipped_grad_mse <- function(beta, X, y, Cap = 1) {
residuals <- as.vector(X %*% beta - y)
gradient_matrix <- X * residuals
gradient_matrix <- apply(gradient_matrix, 1, clip_grad, Cap = Cap)
colMeans(t(gradient_matrix))
}
### AUXILIARY FUNCTIONS
epsilon_from_sigma <- function(noise_sd, n, batch_size, T, delta = 1e-5, cap) {
q <- batch_size / n
alpha <- 1 + noise_sd / (q * sqrt(T)) * sqrt(2 * log(1 / delta))
tau <- 2 * T * (q^2) * alpha * cap^2 / (noise_sd^2)
eps <- tau + (log(1 / delta) + (alpha - 1) * log(1 - 1 / alpha) - log(alpha)) / (alpha - 1)
eps
}
sigma_for_epsilon <- function(eps_target, n, batch_size, T, delta = 1e-5, cap,
bracket = NULL, tol = 1e-10, maxiter = 1000) {
stopifnot(eps_target > 0, n > 0, batch_size > 0, T > 0, delta > 0, delta < 1, cap > 0)
q <- batch_size / n
c_const <- sqrt(2 * log(1 / delta)) / (q * sqrt(T))
A <- 2 * T * (q^2) * cap^2
# Asymptotic large-sigma guess: eps ~ (A * c) / sigma  => sigma ~ (A * c) / eps
sigma0 <- (A * c_const) / eps_target
if (!is.finite(sigma0) || sigma0 <= 0) sigma0 <- 1.0
f <- function(s) epsilon_from_sigma(s, n, batch_size, T, delta, cap) - eps_target
# Auto-bracket if not provided
if (is.null(bracket)) {
lower <- min(1e-8, sigma0 / 10)
upper <- max(1e-3, sigma0 * 10)
# Ensure opposite signs
fL <- f(lower)
fU <- f(upper)
# Increase upper until fU < 0 (epsilon(sigma) below target)
it <- 0
while (fU > 0 && it < 60) {
upper <- upper * 2
fU <- f(upper)
it <- it + 1
}
# Decrease lower until fL > 0 (epsilon(sigma) above target)
it <- 0
while (fL < 0 && it < 60) {
lower <- lower / 2
fL <- f(lower)
it <- it + 1
}
if (!(fL > 0 && fU < 0)) {
stop("Failed to bracket the root automatically; consider providing 'bracket = c(low, high)'.")
}
} else {
stopifnot(length(bracket) == 2, bracket[1] > 0, bracket[2] > bracket[1])
lower <- bracket[1]; upper <- bracket[2]
fL <- f(lower); fU <- f(upper)
if (!(fL > 0 && fU < 0)) {
stop("Provided bracket does not contain a sign change: need f(lower)>0 and f(upper)<0.")
}
}
# Root find
root <- uniroot(f, lower = lower, upper = upper, tol = tol, maxiter = maxiter)
sigma <- root$root
list(
sigma = sigma,
achieved_epsilon = epsilon_from_sigma(sigma, n, batch_size, T, delta, cap),
bracket = c(lower, upper),
f_lower = fL,
f_upper = fU,
iterations = root$iter
)
}
### 3. DP Descent
DP_descent <- function(beta_init, X, y, lr = 0.1, max_iter = 1000,
batch_size = 50, noise_sd = 10, Cap = 1) {
beta <- beta_init
losses <- numeric(max_iter)
params.MSEs <- numeric(max_iter)
for (i in 1:max_iter) {
## Taking a batch
rows <- sample(1:nrow(X), batch_size, replace = TRUE)
X.batch <- X[rows, ]
y.batch <- y[rows]
## Gradient capping + Noisy update
g <- clipped_grad_mse(beta, X.batch, y.batch, Cap = Cap)
noise <- rnorm(length(g), sd = noise_sd * Cap)
beta <- beta - lr * (g + noise)
losses[i] <- mse_loss(beta, X, y)
params.MSEs[i] <- mean((beta - beta_true)^2)
}
list(beta = beta, losses = losses, params.MSEs = params.MSEs)
}
### 4. Barker's Descent
Bark_Descent <- function(beta_init, X, y, lr = 0.1, max_iter = 200,
batch_size = 50, noise_sd = 10) {
beta <- beta_init
losses <- numeric(max_iter)
params.MSEs <- numeric(max_iter)
for (i in 1:max_iter) {
## Taking a batch
rows <- sample(1:nrow(X), batch_size, replace = TRUE)
X.batch <- X[rows, ]
y.batch <- y[rows]
g <- grad_mse(beta, X.batch, y.batch)
g_squash <- (lr*g) / sqrt(1 + (lr*g)^2)  # elementwise
## Noisy update
noise <- rnorm(length(g_squash), sd = noise_sd)
beta <- beta - lr * (g_squash + noise)
losses[i] <- mse_loss(beta, X, y)
params.MSEs[i] <- mean((beta - beta_true)^2)
}
list(beta = beta, losses = losses, params.MSEs = params.MSEs)
}
### 5. Running the models 1000 times
beta_init <- rep(0, p)
lr <- 0.1
max_iter <- 500
noise_sd <- 1
batch_size <- 100
n_loops <- 1000
MSE.params.DP <- numeric(n_loops)
MSE.params.SquashedDP <- numeric(n_loops)
MSE.losses.DP <- numeric(n_loops)
MSE.losses.SquashedDP <- numeric(n_loops)
for (i in 1:n_loops) {
# DP Descent
res_DP <- DP_descent(beta_init, X, y, lr, max_iter, batch_size, noise_sd)
MSE.params.DP[i] <- mean((res_DP$beta - beta_true)^2)
MSE.losses.DP[i] <- mean(res_DP$losses)
# Barker's Descent
res_Bark <- Bark_Descent(beta_init, X, y, lr, max_iter, batch_size, noise_sd)
MSE.params.SquashedDP[i] <- mean((res_Bark$beta - beta_true)^2)
MSE.losses.SquashedDP[i] <- mean(res_Bark$losses)
}
### 7. Plotting the actual models
lr <- 0.5
max_iter <- 2000
noise_sd <- .1
batch_size <- 100
res_DP <- DP_descent(beta_init, X, y, lr, max_iter, batch_size, noise_sd)
res_Bark <- Bark_Descent(beta_init, X, y, lr, max_iter, batch_size, noise_sd)
plot(res_DP$params.MSEs, type = "l", col = "blue", lwd = 2,
xlab = "Iteration", ylab = "Loss",
main = "Params MSEs (DP Vs Barker's Private Descent)",
ylim = c(0, max(c(res_DP$params.MSEs, res_Bark$params.MSEs))))
lines(res_Bark$params.MSEs, col = "red", lwd = 2)
abline(h = mean((beta_true - solve(t(X)%*%X)%*%t(X)%*%y)^2)/2, col = "green", lty = 2)
legend("topright", legend = c("Gradient Descent", "Barker's GD", "Optimal Loss"),
col = c("blue", "red", "green"), lwd = 2)
plot(res_DP$losses, type = "l", col = "blue", lwd = 2,
xlab = "Iteration", ylab = "Loss",
main = "Pred Losses (DP Vs Barker's Private Descent)",
ylim = c(0, max(c(res_DP$losses, res_Bark$losses))))
lines(res_Bark$losses, col = "red", lwd = 2)
abline(h = mean((y - X %*% beta_true)^2)/2, col = "green", lty = 2)
legend("topright", legend = c("Gradient Descent", "Barker's GD", "Optimal Loss"),
col = c("blue", "red", "green"), lwd = 2)
set.seed(123)
### 1. Generate synthetic linear regression dataset
n <- 1e3   # samples
p <- 100     # features
X <- matrix(rnorm(n * p), n, p)
beta_true <- rnorm(p, mean = 2, sd = 1)  # true coefficients
X <- scale(X)  # standardize features
y <- X %*% beta_true + rnorm(n, sd = 0.5)
### 2. Loss function and gradient
mse_loss <- function(beta, X, y) {
residuals <- X %*% beta - y
mean(residuals^2) / 2  # divide by 2 for cleaner gradient form
}
grad_mse <- function(beta, X, y) {
residuals <- X %*% beta - y
t(X) %*% residuals / nrow(X)
}
clip_grad <- function(grad, Cap = 1){
div_factor <- max (Cap, sqrt(sum(grad^2)))
grad <- grad / div_factor
return(grad)
}
clipped_grad_mse <- function(beta, X, y, Cap = 1) {
residuals <- as.vector(X %*% beta - y)
gradient_matrix <- X * residuals
gradient_matrix <- apply(gradient_matrix, 1, clip_grad, Cap = Cap)
colMeans(t(gradient_matrix))
}
### AUXILIARY FUNCTIONS
epsilon_from_sigma <- function(noise_sd, n, batch_size, T, delta = 1e-5, cap) {
q <- batch_size / n
alpha <- 1 + noise_sd / (q * sqrt(T)) * sqrt(2 * log(1 / delta))
tau <- 2 * T * (q^2) * alpha * cap^2 / (noise_sd^2)
eps <- tau + (log(1 / delta) + (alpha - 1) * log(1 - 1 / alpha) - log(alpha)) / (alpha - 1)
eps
}
sigma_for_epsilon <- function(eps_target, n, batch_size, T, delta = 1e-5, cap,
bracket = NULL, tol = 1e-10, maxiter = 1000) {
stopifnot(eps_target > 0, n > 0, batch_size > 0, T > 0, delta > 0, delta < 1, cap > 0)
q <- batch_size / n
c_const <- sqrt(2 * log(1 / delta)) / (q * sqrt(T))
A <- 2 * T * (q^2) * cap^2
# Asymptotic large-sigma guess: eps ~ (A * c) / sigma  => sigma ~ (A * c) / eps
sigma0 <- (A * c_const) / eps_target
if (!is.finite(sigma0) || sigma0 <= 0) sigma0 <- 1.0
f <- function(s) epsilon_from_sigma(s, n, batch_size, T, delta, cap) - eps_target
# Auto-bracket if not provided
if (is.null(bracket)) {
lower <- min(1e-8, sigma0 / 10)
upper <- max(1e-3, sigma0 * 10)
# Ensure opposite signs
fL <- f(lower)
fU <- f(upper)
# Increase upper until fU < 0 (epsilon(sigma) below target)
it <- 0
while (fU > 0 && it < 60) {
upper <- upper * 2
fU <- f(upper)
it <- it + 1
}
# Decrease lower until fL > 0 (epsilon(sigma) above target)
it <- 0
while (fL < 0 && it < 60) {
lower <- lower / 2
fL <- f(lower)
it <- it + 1
}
if (!(fL > 0 && fU < 0)) {
stop("Failed to bracket the root automatically; consider providing 'bracket = c(low, high)'.")
}
} else {
stopifnot(length(bracket) == 2, bracket[1] > 0, bracket[2] > bracket[1])
lower <- bracket[1]; upper <- bracket[2]
fL <- f(lower); fU <- f(upper)
if (!(fL > 0 && fU < 0)) {
stop("Provided bracket does not contain a sign change: need f(lower)>0 and f(upper)<0.")
}
}
# Root find
root <- uniroot(f, lower = lower, upper = upper, tol = tol, maxiter = maxiter)
sigma <- root$root
list(
sigma = sigma,
achieved_epsilon = epsilon_from_sigma(sigma, n, batch_size, T, delta, cap),
bracket = c(lower, upper),
f_lower = fL,
f_upper = fU,
iterations = root$iter
)
}
### 3. DP Descent
DP_descent <- function(beta_init, X, y, lr = 0.1, max_iter = 1000,
batch_size = 50, noise_sd = 10, Cap = 1) {
beta <- beta_init
losses <- numeric(max_iter)
params.MSEs <- numeric(max_iter)
for (i in 1:max_iter) {
## Taking a batch
rows <- sample(1:nrow(X), batch_size, replace = TRUE)
X.batch <- X[rows, ]
y.batch <- y[rows]
## Gradient capping + Noisy update
g <- clipped_grad_mse(beta, X.batch, y.batch, Cap = Cap)
noise <- rnorm(length(g), sd = noise_sd * Cap)
beta <- beta - lr * (g + noise)
losses[i] <- mse_loss(beta, X, y)
params.MSEs[i] <- mean((beta - beta_true)^2)
}
list(beta = beta, losses = losses, params.MSEs = params.MSEs)
}
### 4. Barker's Descent
Bark_Descent <- function(beta_init, X, y, lr = 0.1, max_iter = 200,
batch_size = 50, noise_sd = 10) {
beta <- beta_init
losses <- numeric(max_iter)
params.MSEs <- numeric(max_iter)
for (i in 1:max_iter) {
## Taking a batch
rows <- sample(1:nrow(X), batch_size, replace = TRUE)
X.batch <- X[rows, ]
y.batch <- y[rows]
g <- grad_mse(beta, X.batch, y.batch)
g_squash <- (lr*g) / sqrt(1 + (lr*g)^2)  # elementwise
## Noisy update
noise <- rnorm(length(g_squash), sd = noise_sd)
beta <- beta - lr * (g_squash + noise)
losses[i] <- mse_loss(beta, X, y)
params.MSEs[i] <- mean((beta - beta_true)^2)
}
list(beta = beta, losses = losses, params.MSEs = params.MSEs)
}
### 5. Running the models 1000 times
beta_init <- rep(0, p)
lr <- 0.1
max_iter <- 500
noise_sd <- 1
batch_size <- 100
n_loops <- 1000
MSE.params.DP <- numeric(n_loops)
MSE.params.SquashedDP <- numeric(n_loops)
MSE.losses.DP <- numeric(n_loops)
MSE.losses.SquashedDP <- numeric(n_loops)
for (i in 1:n_loops) {
# DP Descent
res_DP <- DP_descent(beta_init, X, y, lr, max_iter, batch_size, noise_sd)
MSE.params.DP[i] <- mean((res_DP$beta - beta_true)^2)
MSE.losses.DP[i] <- mean(res_DP$losses)
# Barker's Descent
res_Bark <- Bark_Descent(beta_init, X, y, lr, max_iter, batch_size, noise_sd)
MSE.params.SquashedDP[i] <- mean((res_Bark$beta - beta_true)^2)
MSE.losses.SquashedDP[i] <- mean(res_Bark$losses)
}
### 7. Plotting the actual models
lr <- 0.5
max_iter <- 2000
noise_sd <- .1
batch_size <- 100
res_DP <- DP_descent(beta_init, X, y, lr, max_iter, batch_size, noise_sd)
res_Bark <- Bark_Descent(beta_init, X, y, lr, max_iter, batch_size, noise_sd)
plot(res_DP$params.MSEs, type = "l", col = "blue", lwd = 2,
xlab = "Iteration", ylab = "Loss",
main = "Params MSEs (DP Vs Barker's Private Descent)",
ylim = c(0, max(c(res_DP$params.MSEs, res_Bark$params.MSEs))))
lines(res_Bark$params.MSEs, col = "red", lwd = 2)
abline(h = mean((beta_true - solve(t(X)%*%X)%*%t(X)%*%y)^2)/2, col = "green", lty = 2)
legend("topright", legend = c("Gradient Descent", "Barker's GD", "Optimal Loss"),
col = c("blue", "red", "green"), lwd = 2)
plot(res_DP$losses, type = "l", col = "blue", lwd = 2,
xlab = "Iteration", ylab = "Loss",
main = "Pred Losses (DP Vs Barker's Private Descent)",
ylim = c(0, max(c(res_DP$losses, res_Bark$losses))))
lines(res_Bark$losses, col = "red", lwd = 2)
abline(h = mean((y - X %*% beta_true)^2)/2, col = "green", lty = 2)
legend("topright", legend = c("Gradient Descent", "Barker's GD", "Optimal Loss"),
col = c("blue", "red", "green"), lwd = 2)
lr <- 1
max_iter <- 2000
noise_sd <- .1
batch_size <- 100
res_DP <- DP_descent(beta_init, X, y, lr, max_iter, batch_size, noise_sd)
res_Bark <- Bark_Descent(beta_init, X, y, lr, max_iter, batch_size, noise_sd)
plot(res_DP$params.MSEs, type = "l", col = "blue", lwd = 2,
xlab = "Iteration", ylab = "Loss",
main = "Params MSEs (DP Vs Barker's Private Descent)",
ylim = c(0, max(c(res_DP$params.MSEs, res_Bark$params.MSEs))))
lines(res_Bark$params.MSEs, col = "red", lwd = 2)
abline(h = mean((beta_true - solve(t(X)%*%X)%*%t(X)%*%y)^2)/2, col = "green", lty = 2)
legend("topright", legend = c("Gradient Descent", "Barker's GD", "Optimal Loss"),
col = c("blue", "red", "green"), lwd = 2)
plot(res_DP$losses, type = "l", col = "blue", lwd = 2,
xlab = "Iteration", ylab = "Loss",
main = "Pred Losses (DP Vs Barker's Private Descent)",
ylim = c(0, max(c(res_DP$losses, res_Bark$losses))))
lines(res_Bark$losses, col = "red", lwd = 2)
abline(h = mean((y - X %*% beta_true)^2)/2, col = "green", lty = 2)
legend("topright", legend = c("Gradient Descent", "Barker's GD", "Optimal Loss"),
col = c("blue", "red", "green"), lwd = 2)
### Visualising the resuls from LargePcompToN
## load the data for p = 100
data.p100 <- readRDS("results_LargePcompToN_p100.rds")
setwd("~/Research/Random Ideas/EM_for_LAD_Linear_Regression/Codes")
### Visualising the resuls from LargePcompToN
## load the data for p = 100
data.p100 <- readRDS("results_LargePcompToN_p100.rds")
## load the data for p = 200
data.p200 <- readRDS("results_LargePcompToN_p200.rds")
## load the data for p = 400
data.p400 <- readRDS("results_LargePcompToN_p400.rds")
## load the data for p = 800
data.p800 <- readRDS("results_LargePcompToN_p800.rds")
## load the data for p = 1200
data.p1200 <- readRDS("results_LargePcompToN_p1200.rds")
## Making the plots
max_iter <- 100
#for p = 100, 200
par(mfrow = c(2, 1))
plot(1:max_iter, data.p100$MAE.loss, type = "l", col = "blue",
ylim = c(0, max(data.p100$MAE.loss, data.p200$MAE.loss)),
xlab = "Iteration", ylab = "MAE Loss",
main = "MAE Loss over Iterations for p = 100, 200")
lines(1:max_iter, data.p200$MAE.loss, col = "purple2")
legend("topright", legend = c("p = 100", "p = 200"),
col = c("blue", "purple2"), lty = 1)
plot(1:max_iter, data.p100$MAE.params, type = "l", col = "red",
ylim = c(0, max(data.p100$MAE.params, data.p200$MAE.params)),
xlab = "Iteration", ylab = "MAE Loss",
main = "MAE Loss over Iterations for p = 100, 200")
lines(1:max_iter, data.p200$MAE.params, col = "gold")
legend("topright", legend = c("p = 100", "p = 200"),
col = c("red", "gold"), lty = 1)
#for p = 400, 800
par(mfrow = c(2, 1))
plot(1:max_iter, data.p400$MAE.loss, type = "l", col = "blue",
ylim = c(0, max(data.p400$MAE.loss, data.p800$MAE.loss)),
xlab = "Iteration", ylab = "MAE Loss",
main = "MAE Loss over Iterations for p = 400, 800")
lines(1:max_iter, data.p800$MAE.loss, col = "purple2")
legend("topright", legend = c("p = 400", "p = 800"),
col = c("blue", "purple2"), lty = 1)
plot(1:max_iter, data.p400$MAE.params, type = "l", col = "red",
ylim = c(0, max(data.p400$MAE.params, data.p800$MAE.params)),
xlab = "Iteration", ylab = "MAE Loss",
main = "MAE Loss over Iterations for p = 400, 800")
lines(1:max_iter, data.p800$MAE.params, col = "gold")
legend("topright", legend = c("p = 400", "p = 800"),
col = c("red", "gold"), lty = 1)
#for p = 1200
par(mfrow = c(2, 1))
plot(1:max_iter, data.p1200$MAE.loss, type = "l", col = "blue",
ylim = c(0, max(data.p1200$MAE.loss)),
xlab = "Iteration", ylab = "MAE Loss",
main = "MAE Loss over Iterations for p = 1200")
plot(1:max_iter, data.p1200$MAE.params, type = "l", col = "red",
ylim = c(0, max(data.p1200$MAE.params)),
xlab = "Iteration", ylab = "MAE Loss",
main = "MAE Loss over Iterations for p = 1200")
